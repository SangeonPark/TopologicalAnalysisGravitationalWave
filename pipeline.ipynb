{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import GWAnalyzer\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['X', 'y']\n",
      "X shape: (384, 2, 3072)\n",
      "y shape: (384, 1)\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data\"; file_name = \"batch.h5\"\n",
    "\n",
    "with h5py.File(f\"{DATA_DIR}/{file_name}\", \"r\") as f:\n",
    "\n",
    "    # Print the keys (groups and datasets) in the file\n",
    "    print(\"Keys:\", list(f.keys()))\n",
    "    arr = f['X'][:]\n",
    "    labels = f['y'][:]\n",
    "    \n",
    "    \n",
    "    print(\"X shape:\", arr.shape)\n",
    "    print(\"y shape:\", labels.shape)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Topological Features and Save them as npy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector1 = arr[:,0,:]\n",
    "detector2 = arr[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwana = GWAnalyzer(detector1)\n",
    "gwana.obtain_topological_features(True, True)\n",
    "gwana.save_features(os.getcwd(), \"detector1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwana = GWAnalyzer(detector2)\n",
    "gwana.obtain_topological_features(True, True)\n",
    "gwana.save_features(os.getcwd(), \"detector2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangeon/.local/lib/python3.8/site-packages/IPython/html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n",
      "/nobackup/users/sangeon/condas/anaconda3/envs/studies/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "feat_detector1 = np.load(f'{os.path.join(path, \"detector1\")}_topofeatures.npy')\n",
    "feat_detector2 = np.load(f'{os.path.join(path, \"detector2\")}_topofeatures.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat= np.column_stack([feat_detector1, feat_detector2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.35699677e+00, 1.23834451e+00, 6.34000000e+02, ...,\n",
       "        2.86483332e-13, 4.75588372e+03, 2.57630650e+03],\n",
       "       [1.34163579e+00, 1.22745017e+00, 5.86000000e+02, ...,\n",
       "        1.49985396e-13, 4.75588372e+03, 2.65608284e+03],\n",
       "       [1.36242717e+00, 1.25178951e+00, 6.22000000e+02, ...,\n",
       "        3.06250004e-13, 4.75588372e+03, 2.49183743e+03],\n",
       "       ...,\n",
       "       [1.34462065e+00, 1.25264183e+00, 6.35000000e+02, ...,\n",
       "        1.51315183e-13, 4.75588372e+03, 2.48245198e+03],\n",
       "       [1.37408867e+00, 1.25510989e+00, 6.50000000e+02, ...,\n",
       "        1.76925716e-13, 4.75588372e+03, 2.53720045e+03],\n",
       "       [1.36615228e+00, 1.23406257e+00, 6.83000000e+02, ...,\n",
       "        2.11721410e-13, 4.75588372e+03, 2.56379256e+03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = feat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = dataset_split(feat, labels, train_ratio = 0.6, val_ratio = 0.2, test_ratio = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {'train':train_dataset,\n",
    "             'val':val_dataset,\n",
    "             'test':test_dataset,\n",
    "             'predict':test_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TabularDataModule, Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_dm = TabularDataModule(file_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(\"tabular\",\"MLP\", 1e-4, [input_dim, 1, [300,300,150,50,50,20,10,5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CometLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n"
     ]
    }
   ],
   "source": [
    "comet_logger = CometLogger(\n",
    "  api_key=\"CkkrVkSk6Vr2WKlbXIzlkhNlE\",\n",
    "  project_name=\"topogw\",\n",
    "  workspace=\"sangeonpark\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=50, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=os.getcwd(),\n",
    "    filename=\"Test-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback, TQDMProgressBar\n",
    "\n",
    "class PrintCallbacks(Callback):\n",
    "    def on_init_start(self, trainer):\n",
    "        print(\"Starting to init trainer!\")\n",
    "\n",
    "    def on_init_end(self, trainer):\n",
    "        print(\"Trainer is init now\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(\"Training ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "class MyProgressBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_predict_tqdm(self):\n",
    "        bar = super().init_predict_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self):\n",
    "        bar = super().init_test_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pprint \n",
    "  \n",
    "# Get the list of user's \n",
    "env_var = os.environ \n",
    "  \n",
    "# Print the list of user's \n",
    "#print(\"User's Environment variable:\") \n",
    "#pprint.pprint(dict(env_var), width = 1) \n",
    "\n",
    "# ONLY IF YOU ARE IN SLURM ENVIRONMENT\n",
    "os.environ['SLURM_NTASKS_PER_NODE'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(callbacks=[PrintCallbacks(),MyProgressBar(),early_stop_callback,checkpoint_callback],logger=comet_logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.tuner import Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/sangeon/condas/anaconda3/envs/studies/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.com/sangeonpark/topogw/9b5b49182855472dbb82881809dbfbf2\n",
      "\n",
      "/nobackup/users/sangeon/condas/anaconda3/envs/studies/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /home/sangeon/TopologicalAnalysisGravitationalWave exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/nobackup/users/sangeon/condas/anaconda3/envs/studies/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.00 [00:24<00:00,  4.12it/s]\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:24<00:00,  4.14it/s]\n",
      "Learning rate set to 0.10964781961431852\n",
      "Restoring states from the checkpoint path at /home/sangeon/TopologicalAnalysisGravitationalWave/.lr_find_3832310b-a5cd-4f8e-bed7-62ac92c59b1f.ckpt\n",
      "Restored all states from the checkpoint at /home/sangeon/TopologicalAnalysisGravitationalWave/.lr_find_3832310b-a5cd-4f8e-bed7-62ac92c59b1f.ckpt\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/sangeonpark/topogw/9b5b49182855472dbb82881809dbfbf2\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [10] : (0.5292408466339111, 0.7002096772193909)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Created from : pytorch-lightning\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (44.74 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.tuner.lr_finder._LRFinder at 0x2000efed42b0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.lr_find(model, datamodule=tabular_dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/sangeon/condas/anaconda3/envs/studies/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.com/sangeonpark/topogw/f3e3bfe955fb4041a95e9f74ccb938df\n",
      "\n",
      "/nobackup/users/sangeon/condas/anaconda3/envs/studies/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /home/sangeon/TopologicalAnalysisGravitationalWave exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type    | Params\n",
      "---------------------------------------\n",
      "0 | activation | Sigmoid | 0     \n",
      "1 | loss       | BCELoss | 0     \n",
      "2 | layers     | MLP     | 170 K \n",
      "---------------------------------------\n",
      "170 K     Trainable params\n",
      "0         Non-trainable params\n",
      "170 K     Total params\n",
      "0.682     Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/sangeon/condas/anaconda3/envs/studies/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00, 27.89it/s, v_num=38df, train_loss=0.717]\n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  7.46it/s, v_num=38df, train_loss=0.760, val_loss_step=0.715, val_loss_epoch=0.715]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s, v_num=38df, train_loss=0.717, val_loss_step=0.710, val_loss_epoch=0.710]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  7.27it/s, v_num=38df, train_loss=0.720, val_loss_step=0.708, val_loss_epoch=0.708]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  7.72it/s, v_num=38df, train_loss=0.700, val_loss_step=0.706, val_loss_epoch=0.706]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s, v_num=38df, train_loss=0.712, val_loss_step=0.704, val_loss_epoch=0.704]\n",
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s, v_num=38df, train_loss=0.732, val_loss_step=0.703, val_loss_epoch=0.703]\n",
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s, v_num=38df, train_loss=0.727, val_loss_step=0.702, val_loss_epoch=0.702]\n",
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00,  7.86it/s, v_num=38df, train_loss=0.721, val_loss_step=0.701, val_loss_epoch=0.701]\n",
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s, v_num=38df, train_loss=0.718, val_loss_step=0.700, val_loss_epoch=0.700]\n",
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00,  7.42it/s, v_num=38df, train_loss=0.701, val_loss_step=0.698, val_loss_epoch=0.698]\n",
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s, v_num=38df, train_loss=0.706, val_loss_step=0.697, val_loss_epoch=0.697]\n",
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00,  7.72it/s, v_num=38df, train_loss=0.710, val_loss_step=0.696, val_loss_epoch=0.696]\n",
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s, v_num=38df, train_loss=0.678, val_loss_step=0.696, val_loss_epoch=0.696]\n",
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s, v_num=38df, train_loss=0.703, val_loss_step=0.694, val_loss_epoch=0.694]\n",
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s, v_num=38df, train_loss=0.702, val_loss_step=0.692, val_loss_epoch=0.692]\n",
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00,  6.78it/s, v_num=38df, train_loss=0.702, val_loss_step=0.691, val_loss_epoch=0.691]\n",
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s, v_num=38df, train_loss=0.716, val_loss_step=0.690, val_loss_epoch=0.690]\n",
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s, v_num=38df, train_loss=0.699, val_loss_step=0.689, val_loss_epoch=0.689]\n",
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s, v_num=38df, train_loss=0.683, val_loss_step=0.688, val_loss_epoch=0.688]\n",
      "Epoch 20: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s, v_num=38df, train_loss=0.687, val_loss_step=0.687, val_loss_epoch=0.687]\n",
      "Epoch 21: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s, v_num=38df, train_loss=0.699, val_loss_step=0.686, val_loss_epoch=0.686]\n",
      "Epoch 22: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s, v_num=38df, train_loss=0.690, val_loss_step=0.684, val_loss_epoch=0.684]\n",
      "Epoch 23: 100%|██████████| 1/1 [00:00<00:00,  7.72it/s, v_num=38df, train_loss=0.680, val_loss_step=0.682, val_loss_epoch=0.682]\n",
      "Epoch 24: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s, v_num=38df, train_loss=0.693, val_loss_step=0.680, val_loss_epoch=0.680]\n",
      "Epoch 25: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s, v_num=38df, train_loss=0.679, val_loss_step=0.678, val_loss_epoch=0.678]\n",
      "Epoch 26: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s, v_num=38df, train_loss=0.691, val_loss_step=0.677, val_loss_epoch=0.677]\n",
      "Epoch 27: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s, v_num=38df, train_loss=0.680, val_loss_step=0.676, val_loss_epoch=0.676]\n",
      "Epoch 28: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s, v_num=38df, train_loss=0.675, val_loss_step=0.676, val_loss_epoch=0.676]\n",
      "Epoch 29: 100%|██████████| 1/1 [00:00<00:00,  7.63it/s, v_num=38df, train_loss=0.690, val_loss_step=0.678, val_loss_epoch=0.678]\n",
      "Epoch 30: 100%|██████████| 1/1 [00:00<00:00,  7.56it/s, v_num=38df, train_loss=0.665, val_loss_step=0.680, val_loss_epoch=0.680]\n",
      "Epoch 31: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s, v_num=38df, train_loss=0.675, val_loss_step=0.681, val_loss_epoch=0.681]\n",
      "Epoch 32: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s, v_num=38df, train_loss=0.691, val_loss_step=0.681, val_loss_epoch=0.681]\n",
      "Epoch 33: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s, v_num=38df, train_loss=0.671, val_loss_step=0.682, val_loss_epoch=0.682]\n",
      "Epoch 34: 100%|██████████| 1/1 [00:00<00:00,  7.01it/s, v_num=38df, train_loss=0.672, val_loss_step=0.683, val_loss_epoch=0.683]\n",
      "Epoch 35: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s, v_num=38df, train_loss=0.665, val_loss_step=0.684, val_loss_epoch=0.684]\n",
      "Epoch 36: 100%|██████████| 1/1 [00:00<00:00,  7.40it/s, v_num=38df, train_loss=0.679, val_loss_step=0.685, val_loss_epoch=0.685]\n",
      "Epoch 37: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s, v_num=38df, train_loss=0.658, val_loss_step=0.686, val_loss_epoch=0.686]\n",
      "Epoch 38: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s, v_num=38df, train_loss=0.675, val_loss_step=0.687, val_loss_epoch=0.687]\n",
      "Epoch 39: 100%|██████████| 1/1 [00:00<00:00,  7.60it/s, v_num=38df, train_loss=0.660, val_loss_step=0.690, val_loss_epoch=0.690]\n",
      "Epoch 40: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s, v_num=38df, train_loss=0.671, val_loss_step=0.692, val_loss_epoch=0.692]\n",
      "Epoch 41: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s, v_num=38df, train_loss=0.670, val_loss_step=0.694, val_loss_epoch=0.694]\n",
      "Epoch 42: 100%|██████████| 1/1 [00:00<00:00,  7.60it/s, v_num=38df, train_loss=0.662, val_loss_step=0.697, val_loss_epoch=0.697]\n",
      "Epoch 43: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s, v_num=38df, train_loss=0.661, val_loss_step=0.700, val_loss_epoch=0.700]\n",
      "Epoch 44: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s, v_num=38df, train_loss=0.656, val_loss_step=0.703, val_loss_epoch=0.703]\n",
      "Epoch 45: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s, v_num=38df, train_loss=0.662, val_loss_step=0.705, val_loss_epoch=0.705]\n",
      "Epoch 46: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s, v_num=38df, train_loss=0.653, val_loss_step=0.709, val_loss_epoch=0.709]\n",
      "Epoch 47: 100%|██████████| 1/1 [00:00<00:00,  7.73it/s, v_num=38df, train_loss=0.646, val_loss_step=0.709, val_loss_epoch=0.709]\n",
      "Epoch 48: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s, v_num=38df, train_loss=0.658, val_loss_step=0.708, val_loss_epoch=0.708]\n",
      "Epoch 49: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s, v_num=38df, train_loss=0.646, val_loss_step=0.707, val_loss_epoch=0.707]\n",
      "Epoch 50: 100%|██████████| 1/1 [00:00<00:00,  7.58it/s, v_num=38df, train_loss=0.654, val_loss_step=0.708, val_loss_epoch=0.708]\n",
      "Epoch 51: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s, v_num=38df, train_loss=0.670, val_loss_step=0.706, val_loss_epoch=0.706]\n",
      "Epoch 52: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s, v_num=38df, train_loss=0.639, val_loss_step=0.703, val_loss_epoch=0.703]\n",
      "Epoch 53: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s, v_num=38df, train_loss=0.663, val_loss_step=0.702, val_loss_epoch=0.702]\n",
      "Epoch 54: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s, v_num=38df, train_loss=0.654, val_loss_step=0.701, val_loss_epoch=0.701]\n",
      "Epoch 55: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s, v_num=38df, train_loss=0.634, val_loss_step=0.699, val_loss_epoch=0.699]\n",
      "Epoch 56: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s, v_num=38df, train_loss=0.640, val_loss_step=0.699, val_loss_epoch=0.699]\n",
      "Epoch 57: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s, v_num=38df, train_loss=0.634, val_loss_step=0.696, val_loss_epoch=0.696]\n",
      "Epoch 58: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s, v_num=38df, train_loss=0.653, val_loss_step=0.694, val_loss_epoch=0.694]\n",
      "Epoch 59: 100%|██████████| 1/1 [00:00<00:00,  7.30it/s, v_num=38df, train_loss=0.619, val_loss_step=0.693, val_loss_epoch=0.693]\n",
      "Epoch 60: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s, v_num=38df, train_loss=0.643, val_loss_step=0.693, val_loss_epoch=0.693]\n",
      "Epoch 61: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s, v_num=38df, train_loss=0.639, val_loss_step=0.696, val_loss_epoch=0.696]\n",
      "Epoch 62: 100%|██████████| 1/1 [00:00<00:00,  7.78it/s, v_num=38df, train_loss=0.654, val_loss_step=0.697, val_loss_epoch=0.697]\n",
      "Epoch 63: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s, v_num=38df, train_loss=0.614, val_loss_step=0.698, val_loss_epoch=0.698]\n",
      "Epoch 64: 100%|██████████| 1/1 [00:00<00:00,  7.58it/s, v_num=38df, train_loss=0.633, val_loss_step=0.699, val_loss_epoch=0.699]\n",
      "Epoch 65: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s, v_num=38df, train_loss=0.649, val_loss_step=0.700, val_loss_epoch=0.700]\n",
      "Epoch 66: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s, v_num=38df, train_loss=0.620, val_loss_step=0.700, val_loss_epoch=0.700]\n",
      "Epoch 67: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s, v_num=38df, train_loss=0.636, val_loss_step=0.701, val_loss_epoch=0.701]\n",
      "Epoch 68: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s, v_num=38df, train_loss=0.635, val_loss_step=0.700, val_loss_epoch=0.700]\n",
      "Epoch 69: 100%|██████████| 1/1 [00:00<00:00,  7.27it/s, v_num=38df, train_loss=0.621, val_loss_step=0.702, val_loss_epoch=0.702]\n",
      "Epoch 70: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s, v_num=38df, train_loss=0.642, val_loss_step=0.705, val_loss_epoch=0.705]\n",
      "Epoch 71: 100%|██████████| 1/1 [00:00<00:00,  7.75it/s, v_num=38df, train_loss=0.633, val_loss_step=0.705, val_loss_epoch=0.705]\n",
      "Epoch 72: 100%|██████████| 1/1 [00:00<00:00,  7.51it/s, v_num=38df, train_loss=0.622, val_loss_step=0.706, val_loss_epoch=0.706]\n",
      "Epoch 73: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s, v_num=38df, train_loss=0.606, val_loss_step=0.706, val_loss_epoch=0.706]\n",
      "Epoch 74: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s, v_num=38df, train_loss=0.632, val_loss_step=0.708, val_loss_epoch=0.708]\n",
      "Epoch 75: 100%|██████████| 1/1 [00:00<00:00,  7.23it/s, v_num=38df, train_loss=0.620, val_loss_step=0.707, val_loss_epoch=0.707]\n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s, v_num=38df, train_loss=0.611, val_loss_step=0.708, val_loss_epoch=0.708]\n",
      "Training ended|██████████| 1/1 [00:00<00:00,  3.89it/s, v_num=38df, train_loss=0.611, val_loss_step=0.709, val_loss_epoch=0.709]\n",
      "Epoch 76: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s, v_num=38df, train_loss=0.611, val_loss_step=0.709, val_loss_epoch=0.709]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/sangeonpark/topogw/f3e3bfe955fb4041a95e9f74ccb938df\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [8]            : (0.641897976398468, 0.7169286012649536)\n",
      "COMET INFO:     train_loss          : 0.645523726940155\n",
      "COMET INFO:     val_loss_epoch [77] : (0.6759973764419556, 0.7149235606193542)\n",
      "COMET INFO:     val_loss_step [77]  : (0.6759973764419556, 0.7149235606193542)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Created from : pytorch-lightning\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     backbone_type : MLP\n",
      "COMET INFO:     data_type     : tabular\n",
      "COMET INFO:     learning_rate : 0.0001\n",
      "COMET INFO:     modelparams   : [72, 1, [300, 300, 150, 50, 50, 20, 10, 5]]\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (83.12 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=tabular_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.com/sangeonpark/topogw/f3e3bfe955fb4041a95e9f74ccb938df\n",
      "\n",
      "Restoring states from the checkpoint path at /home/sangeon/TopologicalAnalysisGravitationalWave/Test-epoch=26-val_loss=0.68.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/sangeon/TopologicalAnalysisGravitationalWave/Test-epoch=26-val_loss=0.68.ckpt\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/nobackup/users/sangeon/condas/anaconda3/envs/studies/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: -----------------------------------\n",
      "COMET INFO: Comet.ml ExistingExperiment Summary\n",
      "COMET INFO: -----------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/sangeonpark/topogw/f3e3bfe955fb4041a95e9f74ccb938df\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Created from : pytorch-lightning\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     backbone_type : MLP\n",
      "COMET INFO:     data_type     : tabular\n",
      "COMET INFO:     learning_rate : 0.0001\n",
      "COMET INFO:     modelparams   : [72, 1, [300, 300, 150, 50, 50, 20, 10, 5]]\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     model graph : 1\n",
      "COMET INFO: -----------------------------------\n",
      "COMET INFO: Uploading 21 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "predicted_list = trainer.predict(model, tabular_dm, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predicted_list[0][0]\n",
    "label = predicted_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_preds = torch.round(torch.sigmoid(preds)).squeeze()\n",
    "correct = (rounded_preds == label).float() \n",
    "    # Calculate accuracy\n",
    "accuracy = correct.sum() / len(rounded_preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True, False,  True, False,  True,  True,  True, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True, False, False,  True,  True, False,\n",
       "         True,  True, False,  True,  True, False, False,  True,  True,  True,\n",
       "         True, False, False, False,  True,  True, False, False, False, False,\n",
       "        False,  True,  True, False,  True, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False,  True,  True, False,  True, False])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_preds == label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7051)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studies",
   "language": "python",
   "name": "studies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
